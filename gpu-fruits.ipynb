{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")[0]\n",
    "tf.config.experimental.set_memory_growth(gpus, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/quan/fruits-360/Training\n",
      "/root/quan/fruits-360/Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_root_orig = \"/root/quan/fruits-360/Training/\"\n",
    "data_root = pathlib.Path(data_root_orig)\n",
    "print(data_root)\n",
    "\n",
    "# In[78]:\n",
    "\n",
    "test_data_path = \"/root/quan/fruits-360/Test/\"\n",
    "test_path = pathlib.Path(test_data_path)\n",
    "print(test_path)\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "test_image_path = list(test_path.glob('*/*'))\n",
    "test_image_path = [str(path) for path in test_image_path]\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "traing_image_path = list(data_root.glob('*/*'))\n",
    "traing_image_path = [str(path) for path in traing_image_path]\n",
    "label_name = sorted(item.name for item in test_path.glob('*/')\n",
    "                    if item.is_dir())\n",
    "len(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img_raw):\n",
    "    img_tensor = tf.image.decode_jpeg(img_raw, channels=3)\n",
    "    #img_tensor = (img_tensor/127.5) - 1\n",
    "    #\n",
    "\n",
    "    # 格式化0-1\n",
    "    img = tf.image.convert_image_dtype(img_tensor, tf.float32)\n",
    "    tf_fianl = tf.image.resize(img, [100, 100])\n",
    "#     print(img,\"000000\")\n",
    "    return tf_fianl\n",
    "\n",
    "\n",
    "def process_path(file_path):\n",
    "     \n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == label_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def input_fn(batch_size):\n",
    "    list_ds = tf.data.Dataset.from_tensor_slices(traing_image_path)\n",
    "\n",
    "    labeled_ds = list_ds.map(process_path,\n",
    "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return labeled_ds.shuffle(buffer_size=100).batch(batch_size).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def input_fn_test(batch_size):\n",
    "    list_ds = tf.data.Dataset.from_tensor_slices(test_image_path)\n",
    "    labeled_ds = list_ds.map(process_path,\n",
    "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return labeled_ds.shuffle(buffer_size=100).betch(btch_size=100).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = input_fn_test(50)\n",
    "test_dataset = input_fn_test(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = None\n",
    "for i,k in train_dataset.take(1):\n",
    "    pre_data = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpsl8zszff\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpsl8zszff', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator_model = models.Sequential([\n",
    "    Conv2D(16,\n",
    "           3,\n",
    "           padding='same', activation='relu',\n",
    "           input_shape=(100, 100,3)),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(120, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "estimator_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "#model_dir = \"/tmp/note-fruits-tfrecord/\"\n",
    "est_model = tf.keras.estimator.model_to_estimator(keras_model=estimator_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/tmpsl8zszff/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: /tmp/tmpsl8zszff/keras/keras_model.ckpt\n",
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "INFO:tensorflow:Warm-started 14 variables.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpsl8zszff/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.68486905, step = 0\n",
      "INFO:tensorflow:global_step/sec: 41.4198\n",
      "INFO:tensorflow:loss = 0.0, step = 100 (2.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.822\n",
      "INFO:tensorflow:loss = 0.0, step = 200 (2.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.3931\n",
      "INFO:tensorflow:loss = 0.0, step = 300 (2.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.7356\n",
      "INFO:tensorflow:loss = 0.0, step = 400 (2.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.5384\n",
      "INFO:tensorflow:loss = 0.0, step = 500 (2.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.7171\n",
      "INFO:tensorflow:loss = 0.0, step = 600 (2.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.5869\n",
      "INFO:tensorflow:loss = 0.0, step = 700 (2.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.887\n",
      "INFO:tensorflow:loss = 0.0, step = 800 (2.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.3636\n",
      "INFO:tensorflow:loss = 0.0, step = 900 (2.204 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpsl8zszff/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.EstimatorV2 at 0x7f78a0275a58>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_model.train(input_fn=lambda: input_fn(100), steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-03-27T15:04:53Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpsl8zszff/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Inference Time : 0.82771s\n",
      "INFO:tensorflow:Finished evaluation at 2020-03-27-15:04:54\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 1.0, global_step = 1000, loss = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/tmpsl8zszff/model.ckpt-1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0, 'loss': 0.0, 'global_step': 1000}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_model.evaluate(input_fn=lambda: input_fn_test(40), steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_model = model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(100, 100)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "# Compile the model\n",
    "estimator_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "model_dir = \"/tmp/note-fruits/note-tow01\"\n",
    "est_model = tf.keras.estimator.model_to_estimator(keras_model=estimator_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/mobilenet/7/3/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(estimator_model, \"/tmp/mobilenet/7/3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_model.save_weights(\"/tmp/mobilenet/2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported = tf.saved_model.load(\"/tmp/from_estimator/1584945658\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    example = tf.train.Example()\n",
    "    #print(example.features.feature,\"------\")\n",
    "    example.features.feature[\"x\"].float_list.value.extend([x])\n",
    "    return imported.signatures[\"predict\"](\n",
    "    examples=tf.constant([example.SerializeToString()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-04586e20c328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# # print(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print( imported.signatures)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1233\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"classes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "# img,label = process_path(test_image_path[0])\n",
    "# # img =  tf.image.convert_image_dtype(img, tf.float32)\n",
    "# # print(img)\n",
    "# print( imported.signatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpi4s2oaw5\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpi4s2oaw5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpi4s2oaw5/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.6931471, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpi4s2oaw5/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.6931471.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpi4s2oaw5/model.ckpt-1\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/from_estimator/temp-b'1584945658'/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "input_column = tf.feature_column.numeric_column(\"x\")\n",
    "estimator = tf.estimator.LinearClassifier(feature_columns=[input_column])\n",
    "\n",
    "def input_fn():\n",
    "  return tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"x\": [1., 2., 3., 4.]}, [1, 1, 0, 0])).repeat(200).shuffle(64).batch(1000)\n",
    "estimator.train(input_fn)\n",
    "\n",
    "serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "  tf.feature_column.make_parse_example_spec([input_column]))\n",
    "export_path = estimator.export_saved_model(\n",
    "  \"/tmp/from_estimator/\", serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60498 images belonging to 120 classes.\n",
      "Found 20622 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255)\n",
    "train_data_gen  = train_image_generator.flow_from_directory(batch_size=50,\n",
    "                                                           directory=data_root_orig,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(80, 80),\n",
    "                                                           class_mode='categorical')\n",
    "validation_data_gen  = train_image_generator.flow_from_directory(batch_size=50,\n",
    "                                                           directory=test_data_path,\n",
    "                                                           shuffle=True,\n",
    "                        \n",
    "                                                           target_size=(80, 80),\n",
    "                                                           class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 80, 80, 8)         224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 40, 40, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 40, 40, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 40, 40, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 20, 20, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 20, 20, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              3277824   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 3,284,881\n",
      "Trainable params: 3,284,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_new = models.Sequential([\n",
    "    Conv2D(8,3,\n",
    "           padding='same', activation='relu',\n",
    "           input_shape=(80, 80,3)),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(16, 3, padding='same', activation='softmax'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding='same', activation='softmax'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='softmax'),\n",
    "    Dense(1)\n",
    "])\n",
    "model_new.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-708963a1a8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_new' is not defined"
     ]
    }
   ],
   "source": [
    "model_new.fit(train_data_gen, validation_data=validation_data_gen, epochs=10, steps_per_epoch=1000,validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "120/120 [==============================] - 2s 15ms/step - loss: 0.0482 - accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0481942743062973, 0.9916672]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.evaluate(validation_data_gen, steps=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.fit(train_dataset,epochs=10, steps_per_epoch=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.3291801699216692,\n",
       "  0.11861968538334018,\n",
       "  0.07185194444656372,\n",
       "  0.05645804940748775,\n",
       "  0.050796081859230534,\n",
       "  0.04883126484466279,\n",
       "  0.04829229510445262,\n",
       "  0.04820097230702354,\n",
       "  0.04819439266752785,\n",
       "  0.04819427924980626],\n",
       " 'accuracy': [0.99166125,\n",
       "  0.99166125,\n",
       "  0.99166125,\n",
       "  0.99166125,\n",
       "  0.9916613,\n",
       "  0.99166125,\n",
       "  0.9916613,\n",
       "  0.99166125,\n",
       "  0.99166125,\n",
       "  0.99166125],\n",
       " 'val_loss': [0.1688740983605385,\n",
       "  0.0863373689353466,\n",
       "  0.06164244972169399,\n",
       "  0.052707600966095926,\n",
       "  0.04944814294576645,\n",
       "  0.04843253269791603,\n",
       "  0.04821663722395897,\n",
       "  0.04819485545158386,\n",
       "  0.0481942854821682,\n",
       "  0.04819427654147148],\n",
       " 'val_accuracy': [0.9916668,\n",
       "  0.9916668,\n",
       "  0.9916668,\n",
       "  0.9916668,\n",
       "  0.9916668,\n",
       "  0.9916668,\n",
       "  0.9916668,\n",
       "  0.9916668,\n",
       "  0.9916668,\n",
       "  0.9916668]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = model_new.history.history\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = model_new.history.history['accuracy']\n",
    "val_acc = model_new.history.history['val_accuracy']\n",
    "\n",
    "loss = model_new.history.history['loss']\n",
    "val_loss = model_new.history.history['val_loss']\n",
    "\n",
    "epochs_range = range(10)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /tmp/mobilenet/5/assets\n"
     ]
    }
   ],
   "source": [
    "# tf.saved_model.save(model_new, \"/tmp/mobilenet/5/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/mobilenet/7/8/assets\n"
     ]
    }
   ],
   "source": [
    "# loaded = tf.saved_model.load(\"/tmp/mobilenet/5/\")\n",
    "tf.keras.models.save_model(model_new,\"/tmp/mobilenet/7/8\")\n",
    "# print(\"MobileNet has {} trainable variables: {}, ...\".format(\n",
    "#           len(loaded.trainable_variables),\n",
    "#           \", \".join([v.name for v in loaded.trainable_variables[:5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-28 10:51:54.798657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n",
      "2020-03-28 10:51:54.798824: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvrtc.so.10.2: cannot open shared object file: No such file or directory\n",
      "2020-03-28 10:51:54.798834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['conv2d_10_input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 80, 80, 3)\n",
      "        name: serving_default_conv2d_10_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_7'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 80, 80, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_10_input: TensorSpec(shape=(None, 80, 80, 3), dtype=tf.float32, name='conv2d_10_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_10_input: TensorSpec(shape=(None, 80, 80, 3), dtype=tf.float32, name='conv2d_10_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 80, 80, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_10_input: TensorSpec(shape=(None, 80, 80, 3), dtype=tf.float32, name='conv2d_10_input')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 80, 80, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 80, 80, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_10_input: TensorSpec(shape=(None, 80, 80, 3), dtype=tf.float32, name='conv2d_10_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          conv2d_10_input: TensorSpec(shape=(None, 80, 80, 3), dtype=tf.float32, name='conv2d_10_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {\"/tmp/mobilenet/7/8\"} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serving_default']\n",
      "{'dense_5': TensorSpec(shape=(None, 1), dtype=tf.float32, name='dense_5')}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-5ddd71e91472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minfer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"serving_default\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimagenet_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlabeling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "loaded = tf.saved_model.load(\"/tmp/mobilenet/7/7\")\n",
    "print(list(loaded.signatures.keys()))  # [\"serving_default\"]\n",
    "infer = loaded.signatures[\"serving_default\"]\n",
    "print(infer.structured_outputs)\n",
    "#imagenet_labels = np.array(open(label_name).read().splitlines())\n",
    "\n",
    "labeling = infer(tf.constant(x))[pretrained_model.output_names[0]]\n",
    "\n",
    "decoded = label_name[np.argsort(labeling)[0,::-1][:5]+1]\n",
    "\n",
    "print(\"Result after saving and loading:\\n\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 100, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predictions\": [[-4.77908]\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-4.77908])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy\n",
    "import requests\n",
    "\n",
    "test_image1 = \"/root/quan/fruits-360/papers/87761.jpg\"\n",
    "ts = \"/root/quan/fruits-360/Training/Banana/0_100.jpg\"\n",
    "test_image = tf.io.read_file(ts)\n",
    "test_wb = decode_img(test_image)\n",
    "dfp =np.resize(pre_data[0].numpy(), [1,80,80,3])\n",
    "data = json.dumps({\"signature_name\": \"serving_default\",\n",
    "                   \"instances\": dfp.tolist()})\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/mobilenet1:predict',\n",
    "                              data=data, headers=headers)\n",
    "print(json_response.text)\n",
    "predictions = numpy.array(json.loads(json_response.text)[\"predictions\"])\n",
    "\n",
    "#predictions = predictions*100\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple Golden 3'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_name[4]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
